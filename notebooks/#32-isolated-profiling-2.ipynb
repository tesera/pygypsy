{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "In order of priority/time taken\n",
    "\n",
    "1. pandas init dict\n",
    "    - `basal_area_aw_df = pd.DataFrame(columns=['BA_Aw'], index=xrange(max_age))`\n",
    "    - find a faster way to create this data frame\n",
    "    - relax the tolerance for aspen\n",
    "2. pandas set item\n",
    "    - use at method \n",
    "    - http://pandas.pydata.org/pandas-docs/stable/indexing.html#fast-scalar-value-getting-and-setting\n",
    "3. lambdas\n",
    "    - use cython for the gross tot vol and merch vol functions\n",
    "    - might be wise to refactor these first to have conventional names, keyword arguments, and a base implementation to get rid of the boilerplate\n",
    "    - don't be deceived - the callable is a miniscule portion; series.__getitem__ is taking most of the time\n",
    "    - again, using .at here would probably be a significant improvement\n",
    "4. basalareaincremementnonspatialaw\n",
    "    - this is actually slow because of the number of times the BAFromZeroToDataAw function is called as shown above\n",
    "    - relaxing the tolerance may help\n",
    "    - indeed the tolerance is 0.01 * some value while the other factor finder functions have 0.1 tolerance i think\n",
    "    - can also use cython for the increment functions\n",
    "\n",
    "do a profiling run with IO (of reading input data and writing the plot curves to files) in next run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterize what is happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing with df[] or series[] is slow for scalars (lambdas, pandas set)\n",
    "basalareaincrement is running a lot for aw, use the same tolerance as is used for other species\n",
    "\n",
    "merchvol, increment, and gross vol functions use pure python. cython would be effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on the action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use same tolerance for aw as other species\n",
    "- use at instead of [] or ix? - compare these in MWE\n",
    "- creating data frame is slow, maybe because its fromdict. see if this can be improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init from dict and xrange index vs from somethign else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 38.51 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 368 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d = pd.DataFrame(columns=['A'], index=xrange(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.03 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 418 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d = pd.DataFrame(columns=['A'], index=xrange(1000), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.08 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 275 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d = pd.DataFrame({'A': np.zeros(1000)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that dataframe init being called 7000 times because of the aw ba factor finder\n",
    "\n",
    "Maybe it's not worth using a data frame here. use a list or numpy and then convert to dataframe when the factor is found, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.13 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in xrange(5000):\n",
    "    d = pd.DataFrame(columns=['A'], index=xrange(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 14.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in xrange(5000):\n",
    "    d = np.zeros(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the code to see how this can be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy/purepython approach as potential\n",
    "\n",
    "But there's a couple issues for which the code must be examined\n",
    "\n",
    "The problem comes from the following call chain\n",
    "\n",
    "`simulate_forwards_df` (called 1x) ->  \n",
    "`get_factors_for_all_species` (called 10x, 1x per plot) ->  \n",
    "`BAfactorFinder_Aw` (called 2x, 1x per plot that has aw) ->  \n",
    "`BAfromZeroToDataAw` (called 7191 times, most of which in this chain) ->   \n",
    "`DataFrame.__init__` (called 7932 times, most of which in this chain) ...  \n",
    "\n",
    "why does `BAfromZeroToDataAw` create a dataframe? It's good to see the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `simulate_forwards_df` calls `get_factors_for_all_species` and then `BAfromZeroToDataAw` with some parameters and simulation choice of false\n",
    "\n",
    "Note that when `simulation==False`, that is the only time that the list is created. otherwise the list is left empty.\n",
    "\n",
    "Note also that `simulation_choice` defaults to `True` in forward simulation, i.e. for when `BAfromZeroToData__` are called from forward simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_factors_for_all_species` calls factor finder functions for each species, if the species is present, and returns a dict of the factors\n",
    "\n",
    "`BAfactorFinder_Aw` is the main suspect, for sime reason aspen has a harder time converging, so the loop in this function runs many times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It calls `BAfromZeroToDataAw` with `simulation_choice` of `'yes'` and `simulation=True` **BUT IT ONLY USES THE 1ST RETURN VALUE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slow lambdas\n",
    "\n",
    "**below is left here for the record, but the time is actually spent in getitem, not so much in the callables applied, that is an easy fix**\n",
    "\n",
    "With the df init improved by using np array, the next suspect is the lambdas. The method for optimizing is generally to use cython, the functiosn themselves can be examined for opportunities:\n",
    "\n",
    "they are pretty basic - everything is a float.\n",
    "\n",
    "``` python\n",
    "def MerchantableVolumeAw(N_bh_Aw, BA_Aw, topHeight_Aw, StumpDOB_Aw,\n",
    "                         StumpHeight_Aw, TopDib_Aw, Tvol_Aw):\n",
    "    # ...\n",
    "    if N_bh_Aw > 0:\n",
    "        k_Aw = (BA_Aw * 10000.0 / N_bh_Aw)**0.5\n",
    "    else:\n",
    "        k_Aw = 0\n",
    "\n",
    "    if k_Aw > 0 and topHeight_Aw > 0:\n",
    "        b0 = 0.993673\n",
    "        b1 = 923.5825\n",
    "        b2 = -3.96171\n",
    "        b3 = 3.366144\n",
    "        b4 = 0.316236\n",
    "        b5 = 0.968953\n",
    "        b6 = -1.61247\n",
    "        k1 = Tvol_Aw * (k_Aw**b0)\n",
    "        k2 = (b1* (topHeight_Aw**b2) * (StumpDOB_Aw**b3) * (StumpHeight_Aw**b4) * (TopDib_Aw**b5)  * (k_Aw**b6)) + k_Aw\n",
    "        MVol_Aw = k1/k2\n",
    "    else:\n",
    "        MVol_Aw = 0\n",
    "\n",
    "    return MVol_Aw\n",
    "\n",
    "```\n",
    "\n",
    "``` python\n",
    "def GrossTotalVolume_Aw(BA_Aw, topHeight_Aw):\n",
    "    # ...\n",
    "    Tvol_Aw = 0\n",
    "\n",
    "    if topHeight_Aw > 0:\n",
    "        a1 = 0.248718\n",
    "        a2 = 0.98568\n",
    "        a3 = 0.857278\n",
    "        a4 = -24.9961\n",
    "        Tvol_Aw = a1 * (BA_Aw**a2) * (topHeight_Aw**a3) * numpy.exp(1+(a4/((topHeight_Aw**2)+1)))\n",
    "\n",
    "    return Tvol_Aw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings for getitem\n",
    "\n",
    "There's a few ways to get an item from a series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = pd.Series(np.random.randint(0,100, size=(100)), index=['%d' %d for d in xrange(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.05 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 8.4 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 37.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 1.42 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "d.at('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 33.90 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 1.41 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d.loc('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loc` or `at` are faster than `[]` indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revise the code\n",
    "\n",
    "Go on. Do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# git log --since 2016-11-09 --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! git diff HEAD~7 ../gypsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "Do tests still pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# git checkout dev\n",
    "# time gypsy simulate ../private-data/prepped_random_sample_300.csv --output-dir tmp\n",
    "# rm -rfd tmp\n",
    "\n",
    "# real\t8m18.753s\n",
    "# user\t8m8.980s\n",
    "# sys\t0m1.620s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# after factoring dataframe out of zerotodata functions\n",
    "# git checkout -b da080a79200f50d2dda7942c838b7f3cad845280 df-factored-out-zerotodata\n",
    "# time gypsy simulate ../private-data/prepped_random_sample_300.csv --output-dir tmp\n",
    "# rm -rfd tmp\n",
    "\n",
    "# real\t5m51.028s\n",
    "# user\t5m40.130s\n",
    "# sys\t0m1.680s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the data frame init gets a 25% time reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# after using a faster indexing method for the arguments put into the apply functions\n",
    "# git checkout \n",
    "# time gypsy simulate ../private-data/prepped_random_sample_300.csv --output-dir tmp\n",
    "# rm -rfd tmp\n",
    "\n",
    "# real\t6m16.021s\n",
    "# user\t5m59.620s\n",
    "# sys\t0m2.030s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, this actually got worse, although it is a small sample......... if anything i suspect its because we're calling row.at[] instead of assigning the variable outside the loop. It's ok as the code has less reptition, it's a good tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave cython optimization for next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gypsy.forward_simulation import simulate_forwards_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../private-data/prepped_random_sample_300.csv', index_col=0, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file u'forward-sim-2.prof'. \n",
      "\n",
      "*** Profile printout saved to text file u'forward-sim-2.txt'. \n"
     ]
    }
   ],
   "source": [
    "%%prun -D forward-sim-2.prof -T forward-sim-2.txt -q\n",
    "result = simulate_forwards_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         4167158 function calls (4034242 primitive calls) in 33.981 seconds\r\n",
      "\r\n",
      "   Ordered by: internal time\r\n",
      "\r\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n",
      "   492069    6.461    0.000    6.461    0.000 GYPSYNonSpatial.py:428(BasalAreaIncrementNonSpatialAw)\r\n",
      "    90290    2.304    0.000    8.402    0.000 base.py:2116(get_value)\r\n",
      "     7191    2.287    0.000    8.906    0.001 GYPSYNonSpatial.py:956(BAfromZeroToDataAw)\r\n",
      "    90280    1.581    0.000   16.601    0.000 indexing.py:1654(__getitem__)\r\n",
      "   369310    1.435    0.000    2.280    0.000 {isinstance}\r\n"
     ]
    }
   ],
   "source": [
    "!head forward-sim-2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use either of these commands to visualize the profiling\n",
    "\n",
    "```\n",
    "pyprof2calltree -k -i forward-sim-1.prof forward-sim-1.txt\n",
    "\n",
    "# or\n",
    "\n",
    "dc run --service-ports snakeviz notebooks/forward-sim-1.prof\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![definitive reference profile screenshot](forward-sim-1-performance-icicle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2nd iteration performance](forward-sim-2-performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of performance improvements\n",
    "\n",
    "forward_simulation is now 2x faster than last iteration, 8 times in total, due to the changes outlined in the code review section above\n",
    "\n",
    "on my hardware, this takes 1000 plots to ~4 minutes\n",
    "\n",
    "on carol's hardware, this takes 1000 plots to ~13 minutes\n",
    "\n",
    "For 1 million plots, we're looking at 2 to 9 days on desktop hardware\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile with I/O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -rfd gypsy-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = 'gypsy-output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file u'forward-sim-1.prof'. \n",
      "\n",
      "*** Profile printout saved to text file u'forward-sim-1.txt'. \n"
     ]
    }
   ],
   "source": [
    "%%prun -D forward-sim-2.prof -T forward-sim-2.txt -q\n",
    "# restart the kernel first\n",
    "data = pd.read_csv('../private-data/prepped_random_sample_300.csv', index_col=0, nrows=10)\n",
    "result = simulate_forwards_df(data)\n",
    "os.makedirs(output_dir)\n",
    "for plot_id, df in result.items():\n",
    "    filename = '%s.csv' % plot_id\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    df.to_csv(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify new areas to optimize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from last time:\n",
    "    - parallel (3 cores) gets us to 2 - 6 days - save for last\n",
    "    - AWS with 36 cores gets us to 4 - 12 hours ($6.70 - $20.10 USD on a c4.8xlarge instance in US West Region)\n",
    "- now:\n",
    "    - getting item in apply is still slow - vectorize it\n",
    "    - cython for icnrement functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify some means of optimization\n",
    "\n",
    "In order of priority/time taken\n",
    "\n",
    "1.\n",
    "2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
